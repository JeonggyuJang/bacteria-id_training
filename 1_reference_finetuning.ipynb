{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning on the reference dataset\n",
    "In this notebook, we'll demonstrate fine-tuning a pre-trained CNN on the 30-isolate classification task shown in Figure 2. In this example, fine-tuning serves to update the CNN to new measurement parameters. This code illustrates the procedure described in the `CNN architecture & training details` section in the Methods. Note that for speed and clarity, this demo only trains on a single randomly selected train and validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "The first step is to load the fine-tuning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, localtime, strftime\n",
    "t00 = time()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1000) (3000,)\n"
     ]
    }
   ],
   "source": [
    "X_fn = './data/X_finetune.npy'\n",
    "y_fn = './data/y_finetune.npy'\n",
    "X = np.load(X_fn)\n",
    "y = np.load(y_fn)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained CNN\n",
    "Now we set up a ResNet CNN and load weights that we previously trained for the 30-isolate task using the full training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  2\n"
     ]
    }
   ],
   "source": [
    "# CNN parameters\n",
    "layers = 6\n",
    "hidden_size = 100\n",
    "block_size = 2\n",
    "hidden_sizes = [hidden_size] * layers\n",
    "num_blocks = [block_size] * layers\n",
    "input_dim = 1000\n",
    "in_channels = 64\n",
    "n_classes = 30\n",
    "\n",
    "GPU_NUM = 2 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "if device != 'cpu' : cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained weights for demo\n",
    "cnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n",
    "                in_channels=in_channels, n_classes=n_classes)\n",
    "cnn.cuda()\n",
    "cnn.load_state_dict(torch.load(\n",
    "    './pretrained_22-07-17-10-24.ckpt', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "Now we can fine-tune the pre-trained CNN on the new fine-tuning dataset. In the experiments reported in the paper, we fine-tune across 5 randomly selected train and validation splits, but here we show just one split for clarity. We also only train for one epoch here in the interest of time. To train the CNN to convergence, we recommend setting the number of epochs to ~30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import spectral_dataloader\n",
    "from training import run_epoch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val split\n",
    "We split the fine-tuning dataset into train and validation sets. We randomly sample 10% of the dataset to use as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1000) (3000,)\n"
     ]
    }
   ],
   "source": [
    "p_val = 0.1\n",
    "n_val = int(3000 * p_val)\n",
    "idx_tr = list(range(3000))\n",
    "np.random.shuffle(idx_tr)\n",
    "idx_val = idx_tr[:n_val]\n",
    "idx_tr = idx_tr[n_val:]\n",
    "\n",
    "# Fine-tune CNN\n",
    "epochs = 30 # Change this number to ~30 for full training\n",
    "batch_size = 500\n",
    "t0 = time()\n",
    "# Set up Adam optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "# Set up dataloaders\n",
    "print(X.shape, y.shape)\n",
    "dl_tr = spectral_dataloader(X, y, idxs=idx_tr,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "dl_val = spectral_dataloader(X, y, idxs=idx_val,\n",
    "    batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fine-tune!\n",
      " Epoch 1: 0.01s\n",
      "  Train acc: 65.41\n",
      "  Val acc  : 75.33\n",
      " Epoch 2: 2.57s\n",
      "  Train acc: 81.96\n",
      "  Val acc  : 85.33\n",
      " Epoch 3: 4.85s\n",
      "  Train acc: 87.44\n",
      "  Val acc  : 88.33\n",
      " Epoch 4: 7.06s\n",
      "  Train acc: 92.26\n",
      "  Val acc  : 88.67\n",
      " Epoch 5: 9.33s\n",
      "  Train acc: 94.52\n",
      "  Val acc  : 91.67\n",
      " Epoch 6: 11.58s\n",
      "  Train acc: 96.89\n",
      "  Val acc  : 94.00\n",
      " Epoch 7: 13.90s\n",
      "  Train acc: 98.19\n",
      "  Val acc  : 92.67\n",
      " Epoch 8: 16.17s\n",
      "  Train acc: 98.96\n",
      "  Val acc  : 93.33\n",
      " Epoch 9: 18.48s\n",
      "  Train acc: 99.63\n",
      "  Val acc  : 94.00\n",
      " Epoch 10: 20.79s\n",
      "  Train acc: 99.81\n",
      "  Val acc  : 93.33\n",
      " Epoch 11: 23.15s\n",
      "  Train acc: 100.00\n",
      "  Val acc  : 94.00\n",
      "Finished after 11 epochs!\n",
      "\n",
      " completed in: 29.32s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tune CNN for first fold\n",
    "best_val = 0\n",
    "no_improvement = 0\n",
    "max_no_improvement = 5\n",
    "print('Starting Fine-tune!')\n",
    "for epoch in range(epochs):\n",
    "    print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\n",
    "    # Train\n",
    "    acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n",
    "        training=True, optimizer=optimizer)\n",
    "    print('  Train acc: {:0.2f}'.format(acc_tr))\n",
    "    # Val\n",
    "    acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\n",
    "        training=False, optimizer=optimizer)\n",
    "    print('  Val acc  : {:0.2f}'.format(acc_val))\n",
    "    # Check performance for early stopping\n",
    "    if acc_val > best_val or epoch == 0:\n",
    "        best_val = acc_val\n",
    "        no_improvement = 0\n",
    "        best_model = cnn\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    if no_improvement >= max_no_improvement:\n",
    "        print('Finished after {} epochs!'.format(epoch+1))\n",
    "        break\n",
    "\n",
    "torch.save(best_model.state_dict()\n",
    "           ,'finetuned_'+strftime('%y-%m-%d-%H-%M',localtime(t00))+'.ckpt')\n",
    "\n",
    "print('\\n completed in: {:0.2f}s'.format(time()-t00))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies seen here are not representative of the accuracies achieved when training on the full dataset until convergence. To do this, increase the number of epoches. This code demonstrates how a pre-trained CNN can be fine-tuned and evaluated using randomly selected train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import get_predictions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1000) (3000,)\n"
     ]
    }
   ],
   "source": [
    "X_test = './data/X_test.npy'\n",
    "y_test = './data/y_test.npy'\n",
    "X = np.load(X_test)\n",
    "y = np.load(y_test)\n",
    "print(X.shape, y.shape)\n",
    "dl_test = spectral_dataloader(X, y, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy = 100.0%\n",
      "validation accuracy = 94.0%\n",
      "test accuracy = 83.13333333333334%\n"
     ]
    }
   ],
   "source": [
    "preds_tr, corr_tr = get_predictions(best_model,dl_tr,cuda)\n",
    "preds_val, corr_val = get_predictions(best_model,dl_val,cuda)\n",
    "preds_test, corr_test = get_predictions(best_model,dl_test,cuda)\n",
    "print(\"train accuracy = {}%\".format(corr_tr/2700*100))\n",
    "print(\"validation accuracy = {}%\".format(corr_val/300*100))\n",
    "print(\"test accuracy = {}%\".format(corr_test/3000*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
